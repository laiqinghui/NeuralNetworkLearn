{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_NEURON = 50\n",
    "NUM_FEATURES = 7\n",
    "\n",
    "\n",
    "keep_prob = 0.8\n",
    "learning_rate = 10**-3\n",
    "beta = 10**-3\n",
    "\n",
    "epochs = 1500\n",
    "batch_size = 8\n",
    "\n",
    "df = pd.read_csv(\"data/admission_predict.csv\")\n",
    "df = df.drop(['Serial No.'], axis=1)\n",
    "\n",
    "y = df['Chance of Admit'].to_numpy()\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "\n",
    "#Optimized one \n",
    "features = df.drop([\"Chance of Admit\"], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    #hidden layer 1\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) #activation function\n",
    "    layer_1 = tf.nn.dropout(layer_1, rate=(1-keep_prob))\n",
    "    \n",
    "    #hidden layer 2\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) #activation function\n",
    "    layer_2 = tf.nn.dropout(layer_2, rate=(1-keep_prob))\n",
    "\n",
    "    \n",
    "    #hidden layer 3\n",
    "#     layer_3 = tf.add(tf.matmul(x, weights['h3']), biases['b3'])\n",
    "#     layer_3 = tf.nn.relu(layer_3) #activation function\n",
    "    \n",
    "    #linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return (out_layer)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([NUM_FEATURES, NUM_NEURON], seed=1337, stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32)),\n",
    "    'h2': tf.Variable(tf.truncated_normal([NUM_NEURON, NUM_NEURON], seed=1337, stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32)),\n",
    "    'out': tf.Variable(tf.truncated_normal([NUM_NEURON, 1], seed=1337, stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32))# 1 ouput label\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([NUM_NEURON], seed=1337)),\n",
    "    'b2': tf.Variable(tf.random_normal([NUM_NEURON], seed=1337)),\n",
    "    'out': tf.Variable(tf.random_normal([1], seed=1337))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "tf.set_random_seed(1337)\n",
    "\n",
    "#Create model\n",
    "X = tf.placeholder(tf.float32, [None, NUM_FEATURES])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, y, test_size=0.3)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "#predicted value\n",
    "Y_hat = neural_net(X)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "loss = tf.reduce_mean(tf.square(Y - Y_hat)) \n",
    "reg = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2']) +  tf.nn.l2_loss(weights['out'])\n",
    "train_op = optimizer.minimize(loss + reg * beta)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_train_err = []\n",
    "    test_err = []\n",
    "    for i in range(epochs):\n",
    "\n",
    "        #Batch training\n",
    "        #pick random batch_size data points from x_train\n",
    "        rand_index = np.random.choice(x_train.shape[0], size=batch_size)\n",
    "        x_batch = x_train[rand_index]\n",
    "        y_batch = y_train[rand_index]\n",
    "\n",
    "        train_op.run(feed_dict={X:x_batch, Y:y_batch})\n",
    "      \n",
    "\n",
    "        if(i%10==0):\n",
    "            batch_train_err.append(loss.eval(feed_dict={X: x_batch, Y: y_batch}))\n",
    "            test_err.append(loss.eval(feed_dict={X: x_test, Y: y_test}))   \n",
    "            print (\"epoch %d: train err %g, test error %g \" % (i, batch_train_err[-1], test_err[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q1a plot train err against test err\n",
    "plt.plot(range(0, epochs, 10), batch_train_err, color='blue', label='Train Error')\n",
    "plt.plot(range(0, epochs, 10), test_err, color ='red', label='Test Error')\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
