{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "\n",
    "if os.environ.get('DISPLAY','') == '':\n",
    "    print('no display found. Using non-interactive Agg backend')\n",
    "    mpl.use('Agg')\n",
    "    \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 100\n",
    "N_FILTERS = 10\n",
    "FILTER_SHAPE1 = [20, 20]\n",
    "FILTER_SHAPE2 = [20, 1]\n",
    "\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "MAX_LABEL = 15\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "EMBEDDING_SIZE = 20\n",
    "\n",
    "no_epochs = 1000\n",
    "lr = 0.01\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "seed = 10\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data_words():\n",
    "  \n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "    with open('train_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            x_train.append(row[1])\n",
    "            y_train.append(int(row[0]))\n",
    "\n",
    "    with open('test_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            x_test.append(row[1])\n",
    "            y_test.append(int(row[0]))\n",
    "  \n",
    "    x_train = pandas.Series(x_train)\n",
    "    y_train = pandas.Series(y_train)\n",
    "    x_test = pandas.Series(x_test)\n",
    "    y_test = pandas.Series(y_test)\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "  \n",
    "  \n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      MAX_DOCUMENT_LENGTH)\n",
    "    \n",
    "    x_transform_train = vocab_processor.fit_transform(x_train)\n",
    "    x_transform_test = vocab_processor.transform(x_test)\n",
    "    \n",
    "    x_train = np.array(list(x_transform_train))\n",
    "    print(\"len(list(x_transform_train)): \", len(list(x_transform_train)))\n",
    "    print(\"x_train.shape\", x_train.shape)\n",
    "    x_test = np.array(list(x_transform_test))\n",
    "    \n",
    "    no_words = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % no_words)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, no_words\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def char_cnn_model(x):\n",
    "\n",
    "    #Embedded layer of size 20\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        x, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        input_layer,\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=FILTER_SHAPE1,\n",
    "        padding='VALID',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        conv1,\n",
    "        pool_size=POOLING_WINDOW,\n",
    "        strides=POOLING_STRIDE,\n",
    "        padding='SAME')\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool1,\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=FILTER_SHAPE2,\n",
    "        padding='VALID',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        conv2,\n",
    "        pool_size=POOLING_WINDOW,\n",
    "        strides=POOLING_STRIDE,\n",
    "        padding='SAME')\n",
    "\n",
    "    pool2 = tf.squeeze(tf.reduce_max(pool2, 1), squeeze_dims=[1]) \n",
    "\n",
    "    logits = tf.layers.dense(pool2, MAX_LABEL, activation=None)\n",
    "\n",
    "    return input_layer, logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(list(x_transform_train)):  0\n",
      "x_train.shape (5600, 100)\n",
      "Total words: 9155\n",
      "5600\n",
      "700\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 100, 20]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-65425f105b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-0bb3776fc089>\u001b[0m in \u001b[0;36mchar_cnn_model\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFILTER_SHAPE1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'VALID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         activation=tf.nn.relu)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     pool1 = tf.layers.max_pooling2d(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    422\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m       _scope=name)\n\u001b[1;32m--> 424\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \"\"\"\n\u001b[1;32m-> 1479\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# the corresponding TF subgraph inside `backend.get_graph()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 586\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 100, 20]"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_train, y_train, x_test, y_test, n_words = read_data_words()\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.int64, [None, MAX_DOCUMENT_LENGTH])\n",
    "y_ = tf.placeholder(tf.int64)\n",
    "\n",
    "inputs, logits = char_cnn_model(x)\n",
    "\n",
    "# Optimizer\n",
    "entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(y_, MAX_LABEL), logits=logits))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(entropy)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(tf.one_hot(y_, MAX_LABEL), 1))\n",
    "correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # training\n",
    "    loss = []\n",
    "    test_acc = []\n",
    "\n",
    "    for e in range(no_epochs):\n",
    "\n",
    "        rand_index = np.random.choice(x_train.shape[0], size=BATCH_SIZE)\n",
    "        x_batch = x_train[rand_index]\n",
    "        y_batch = y_train[rand_index]\n",
    "\n",
    "        if(e==0):\n",
    "            print(x_train.shape)\n",
    "        word_vectors_, word_list_, _, loss_  = sess.run([word_vectors, word_list, train_op, entropy], {x: x_train, y_: y_train})\n",
    "        loss.append(loss_)\n",
    "\n",
    "\n",
    "        test_acc.append(accuracy.eval(feed_dict={x: x_test, y_: y_test}))\n",
    "\n",
    "\n",
    "        if e%1 == 0:\n",
    "            print('iter: %d, entropy: %g, accuracy: %g'%(e, loss[e], test_acc[e]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "# plt.plot(range(no_epochs), loss, 'r' ,label=\"Training Loss\")\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('loss')\n",
    "# plt.title('Training Loss')\n",
    "# #plt.savefig('q1figs/loss.png')\n",
    "# plt.savefig(\"q1figs/loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(2)\n",
    "# plt.plot(range(no_epochs), test_acc, 'g' ,label=\"Test Accuracy\")\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Test Accuracy')\n",
    "# #plt.savefig('q1figs/loss.png')\n",
    "# plt.savefig(\"q1figs/accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
